{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First steps for intake-ESGF-Catalog (https://intake-esgf.readthedocs.io/en/latest/quickstart.html)\n",
    "from intake_esgf import ESGFCatalog\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import netCDF4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined all functions used in this script\n",
    "\n",
    "# Placeholder function to generate a sample of weights\n",
    "def generate_sample_of_weights(num_models):\n",
    "    \"\"\"\n",
    "    Generate a sample of weights that are non-negative and sum up to 1.\n",
    "    Args:\n",
    "    num_models (int): The number of models (and hence the number of weights to generate)\n",
    "    Returns:\n",
    "    numpy.ndarray: An array of weights that sum up to 1.\n",
    "    \"\"\"\n",
    "    # Generate uniform samples which naturally fall between 0 and 1\n",
    "    weights = np.random.uniform(0, 1, size=num_models)\n",
    "    weights = weights **4\n",
    "    # Normalize the weights so they sum up to 1\n",
    "    normalized_weights = weights / np.sum(weights)\n",
    "    return normalized_weights\n",
    "\n",
    "# Placeholder for 'models' which would be an array of models.\n",
    "# Here we assume each model is a function that predicts an output given an input.\n",
    "# In the actual use case, these should be replaced with real predictive models.\n",
    "def model_predictions(models_1, models_2, models_3, models_4, models_5, models_6, models_7, models_8, models_9, models_10, weights_norm):\n",
    "    model_mean_weighted = (weights_norm[0]*models_1) + (weights_norm[1]*models_2) + (weights_norm[2]*models_3) + (weights_norm[3]* models_4) + (weights_norm[4]*models_5) + (weights_norm[5]*models_6) + (weights_norm[6]*models_7) + (weights_norm[7]*models_8) + (weights_norm[8]*models_9) + (weights_norm[9]*models_10)\n",
    "    return model_mean_weighted\n",
    "\n",
    "# function for shifiting lon according to Bharat's code\n",
    "def shift_lon(model):\n",
    "    # First shift the lon dimension for the model average\n",
    "    ds_tmp = model.copy(deep=True)\n",
    "    ds_tmp['lon'] = ds_tmp['lon'] - 180\n",
    "\n",
    "    gpp_reshape = np.zeros(ds_tmp.shape)\n",
    "    gpp_reshape[:,ds_tmp['lon'].size//2:] = ds_tmp[:,:ds_tmp['lon'].size//2].data\n",
    "    gpp_reshape[:,:ds_tmp['lon'].size//2] = ds_tmp[:,ds_tmp['lon'].size//2:].data\n",
    "    ds_tmp.data = gpp_reshape\n",
    "    return ds_tmp \n",
    "\n",
    "# Create function that will do RMSE for each seperate model\n",
    "\n",
    "def get_RMSE(model, obs):\n",
    "    # Step 2: Compute the Difference\n",
    "    difference = model - obs\n",
    "    # Step 3: Square the Difference\n",
    "    squared_difference = difference ** 2\n",
    "    # Step 4: Compute the Mean Squared Error\n",
    "    mse = squared_difference.mean(dim=['lat', 'lon'])\n",
    "    # Step 5: Take the Square Root to get RMSE\n",
    "    rmse  = np.sqrt(mse)\n",
    "    rmse_weighted = rmse.values.item()\n",
    "\n",
    "    return rmse_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform a search() to populate the catalog.\n"
     ]
    }
   ],
   "source": [
    "#Populate the Catalog - bringing in nothing from the catalog\n",
    "cat = ESGFCatalog()\n",
    "print(cat)  # <-- nothing to see here yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb9f5ba7a824baf9cd15f0d7aa05d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   Searching indices:   0%|          |0/2 [       ?index/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Summary information for 10 results:\n",
       "member_id                                      [r1i1p1f1, r1i1p1f2]\n",
       "experiment_id                                          [historical]\n",
       "variable_id                                                   [gpp]\n",
       "mip_era                                                     [CMIP6]\n",
       "table_id                                                     [Lmon]\n",
       "institution_id    [NOAA-GFDL, NCC, IPSL, CSIRO, NCAR, BCC, MOHC,...\n",
       "source_id         [GFDL-ESM4, NorESM2-LM, IPSL-CM6A-LR, ACCESS-E...\n",
       "grid_label                                            [gr1, gn, gr]\n",
       "activity_drs                                                 [CMIP]\n",
       "project                                                     [CMIP6]\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import selected models from Intake-ESGF Catalog for selected variable\n",
    "\n",
    "models = [\"ACCESS-ESM1-5\",\"IPSL-CM6A-LR\",\"CESM2\", \"UKESM1-0-LL\",\"BCC-CSM2-MR\",\"MPI-ESM1-2-HR\",\"CanESM5\",\"GFDL-ESM4\",\"NorESM2-LM\", \"MIROC-ES2L\"]\n",
    "\n",
    "cat.search(\n",
    "    experiment_id=\"historical\",\n",
    "    source_id= models,\n",
    "    frequency=\"mon\",\n",
    "    variable_id=[\"gpp\"],\n",
    ")\n",
    "cat.remove_ensembles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c1569497ac4f90ae98e9c634d1fe95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Get file information:   0%|          |0/2 [       ?index/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d9ece2dc9149b2a870b06083d91ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding cell measures:   0%|          |0/10 [     ?dataset/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Obtaining the datasets and loading it into a dictionary (putting it in the shopping cart)\n",
    "\n",
    "dsd = cat.to_dataset_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['r1i1p1f1.MPI-M.MPI-ESM1-2-HR.gn', 'r1i1p1f1.BCC.BCC-CSM2-MR.gn', 'r1i1p1f2.MIROC.MIROC-ES2L.gn', 'r1i1p1f1.IPSL.IPSL-CM6A-LR.gr', 'r1i1p1f1.NCC.NorESM2-LM.gn', 'r1i1p1f1.CCCma.CanESM5.gn', 'r1i1p1f2.MOHC.UKESM1-0-LL.gn', 'r1i1p1f1.NCAR.CESM2.gn', 'r1i1p1f1.NOAA-GFDL.GFDL-ESM4.gr1', 'r1i1p1f1.CSIRO.ACCESS-ESM1-5.gn'])\n"
     ]
    }
   ],
   "source": [
    "#printing variable keys to see how variable names are set up\n",
    "print(dsd.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'time' (time: 407)> Size: 3kB\n",
      "array([cftime.Datetime360Day(1980, 1, 16, 0, 0, 0, 0, has_year_zero=True),\n",
      "       cftime.Datetime360Day(1980, 2, 16, 0, 0, 0, 0, has_year_zero=True),\n",
      "       cftime.Datetime360Day(1980, 3, 16, 0, 0, 0, 0, has_year_zero=True), ...,\n",
      "       cftime.Datetime360Day(2013, 9, 16, 0, 0, 0, 0, has_year_zero=True),\n",
      "       cftime.Datetime360Day(2013, 10, 16, 0, 0, 0, 0, has_year_zero=True),\n",
      "       cftime.Datetime360Day(2013, 11, 16, 0, 0, 0, 0, has_year_zero=True)],\n",
      "      dtype=object)\n",
      "Coordinates:\n",
      "  * time     (time) object 3kB 1980-01-16 00:00:00 ... 2013-11-16 00:00:00\n",
      "    type     |S4 4B ...\n",
      "Attributes:\n",
      "    bounds:         time_bnds\n",
      "    axis:           T\n",
      "    long_name:      time\n",
      "    standard_name:  time\n"
     ]
    }
   ],
   "source": [
    "# Define models for selected time periods and fixed units (make sure that model names match the printed dict_key (above))\n",
    "\n",
    "model_1 = dsd[\"r1i1p1f1.CSIRO.ACCESS-ESM1-5.gn\"][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400 * 1000 # .mean(dim=\"time\") * 86400 * 1000\n",
    "model_2 = dsd[\"r1i1p1f1.BCC.BCC-CSM2-MR.gn\"][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400  * 1000# .mean(dim=\"time\") * 86400 * 1000\n",
    "model_3 = dsd[\"r1i1p1f1.CCCma.CanESM5.gn\"][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400  * 1000 # .mean(dim=\"time\") * 86400 * 1000\n",
    "model_4 = dsd[\"r1i1p1f1.NCAR.CESM2.gn\"][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400  * 1000 # .mean(dim=\"time\") * 86400 * 1000\n",
    "model_5 = dsd[\"r1i1p1f1.NOAA-GFDL.GFDL-ESM4.gr1\"][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400  * 1000 # .mean(dim=\"time\") * 86400 * 1000\n",
    "model_6 = dsd[\"r1i1p1f1.IPSL.IPSL-CM6A-LR.gr\"][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400 * 1000 # .mean(dim=\"time\") * 86400 * 1000\n",
    "model_7 = dsd[\"r1i1p1f2.MIROC.MIROC-ES2L.gn\"][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400 * 1000 # .mean(dim=\"time\") * 86400 * 1000\n",
    "model_8 = dsd[\"r1i1p1f1.MPI-M.MPI-ESM1-2-HR.gn\"][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400 * 1000 # .mean(dim=\"time\") * 86400 * 1000\n",
    "model_9 = dsd[\"r1i1p1f1.NCC.NorESM2-LM.gn\"][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400 * 1000 # .mean(dim=\"time\") * 86400 * 1000\n",
    "model_10 = dsd[\"r1i1p1f2.MOHC.UKESM1-0-LL.gn\"][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400* 1000 # .mean(dim=\"time\") * 86400 * 1000\n",
    "\n",
    "print(model_10[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Interpolation/Regridding to match lowest resolution model (CanESM5):\n",
    "\n",
    "#Extract lat/lon grid from CanESM5\n",
    "lat_target = model_3.lat.values\n",
    "lon_target = model_3.lon.values\n",
    "\n",
    "# Regrid each model and take long (not longtitude) term mean \n",
    "gpp_model_1_Regridded = model_1.interp(lat=lat_target, lon=lon_target)\n",
    "gpp_model_1_Regridded_mean = gpp_model_1_Regridded.mean(dim=\"time\")\n",
    "\n",
    "gpp_model_2_Regridded = model_2.interp(lat=lat_target, lon=lon_target)\n",
    "gpp_model_2_Regridded_mean = gpp_model_2_Regridded.mean(dim=\"time\")\n",
    "\n",
    "gpp_model_3_Regridded = model_3.interp(lat=lat_target, lon=lon_target)\n",
    "gpp_model_3_Regridded_mean = gpp_model_3_Regridded.mean(dim=\"time\")\n",
    "\n",
    "gpp_model_4_Regridded = model_4.interp(lat=lat_target, lon=lon_target)\n",
    "gpp_model_4_Regridded_mean = gpp_model_4_Regridded.mean(dim=\"time\")\n",
    "\n",
    "gpp_model_5_Regridded = model_5.interp(lat=lat_target, lon=lon_target)\n",
    "gpp_model_5_Regridded_mean = gpp_model_5_Regridded.mean(dim=\"time\")\n",
    "\n",
    "gpp_model_6_Regridded = model_6.interp(lat=lat_target, lon=lon_target)\n",
    "gpp_model_6_Regridded_mean = gpp_model_6_Regridded.mean(dim=\"time\")\n",
    "\n",
    "gpp_model_7_Regridded = model_7.interp(lat=lat_target, lon=lon_target)\n",
    "gpp_model_7_Regridded_mean = gpp_model_7_Regridded.mean(dim=\"time\")\n",
    "\n",
    "gpp_model_8_Regridded = model_8.interp(lat=lat_target, lon=lon_target)\n",
    "gpp_model_8_Regridded_mean = gpp_model_8_Regridded.mean(dim=\"time\")\n",
    "\n",
    "gpp_model_9_Regridded = model_9.interp(lat=lat_target, lon=lon_target)\n",
    "gpp_model_9_Regridded_mean = gpp_model_9_Regridded.mean(dim=\"time\")\n",
    "\n",
    "gpp_model_10_Regridded = model_10.interp(lat=lat_target, lon=lon_target)\n",
    "gpp_model_10_Regridded_mean = gpp_model_10_Regridded.mean(dim=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift every model to proper coordinates\n",
    "gpp_model_1_Regridded_mean_shifted = shift_lon(gpp_model_1_Regridded_mean)\n",
    "gpp_model_2_Regridded_mean_shifted = shift_lon(gpp_model_2_Regridded_mean)\n",
    "gpp_model_3_Regridded_mean_shifted = shift_lon(gpp_model_3_Regridded_mean)\n",
    "gpp_model_4_Regridded_mean_shifted = shift_lon(gpp_model_4_Regridded_mean)\n",
    "gpp_model_5_Regridded_mean_shifted = shift_lon(gpp_model_5_Regridded_mean)\n",
    "gpp_model_6_Regridded_mean_shifted = shift_lon(gpp_model_6_Regridded_mean)\n",
    "gpp_model_7_Regridded_mean_shifted = shift_lon(gpp_model_7_Regridded_mean)\n",
    "gpp_model_8_Regridded_mean_shifted = shift_lon(gpp_model_8_Regridded_mean)\n",
    "gpp_model_9_Regridded_mean_shifted = shift_lon(gpp_model_9_Regridded_mean)\n",
    "gpp_model_10_Regridded_mean_shifted = shift_lon(gpp_model_10_Regridded_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the observation \n",
    "\n",
    "# Open the NetCDF file as an xarray Dataset\n",
    "ds = xr.open_dataset('/Users/6i0/Documents/Data/gpp_WECANN.nc')\n",
    "\n",
    "# Access the GPP variable from the Dataset\n",
    "gpp_data = ds['gpp']\n",
    "gpp_data_mean = gpp_data.mean(dim=\"time\")\n",
    "lat = ds['lat']\n",
    "lon = ds['lon']\n",
    "time = ds['time']\n",
    "\n",
    "#gpp_data_Regridded_mean = gpp_data_mean\n",
    "\n",
    "lat_target = model_3.lat.values\n",
    "lon_target = model_3.lon.values -180\n",
    "\n",
    "# Correctly using ds_emean for interpolation\n",
    "gpp_data_Regridded = gpp_data.interp(lat=lat_target, lon=lon_target)\n",
    "gpp_data_Regridded_mean = gpp_data_Regridded.mean(dim=\"time\")\n",
    "\n",
    "#Shift data\n",
    "gpp_data_Regridded_mean_shifted = shift_lon(gpp_data_Regridded_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6002035355286313,\n",
       " 25,\n",
       " array([7.49133742e-02, 2.28896631e-02, 2.57622127e-04, 2.90522406e-05,\n",
       "        1.39155674e-01, 4.21988054e-01, 1.27359052e-01, 1.98152371e-01,\n",
       "        3.83363315e-03, 1.14215042e-02]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BMA Implementation:\n",
    "\n",
    "# Placeholder for actual observed data (data_obs)\n",
    "data_obs = gpp_data_Regridded_mean\n",
    "\n",
    "# Number of models we are combining\n",
    "num_models = 10 \n",
    "# Number of BMA Samples \n",
    "n_samples= 100\n",
    "\n",
    "# Initialize arrays to hold weights, weighted averages and RMSE\n",
    "new_weights = np.zeros((n_samples, num_models))\n",
    "weighted_avgs = np.zeros((n_samples, len(data_obs)))\n",
    "rmse_weighted = np.zeros(n_samples)\n",
    "\n",
    "# Loop to generate weights, calculate weighted averages and RMSE\n",
    "for i in range(n_samples):\n",
    "    weights = generate_sample_of_weights(num_models)\n",
    "    new_weights[i, :] = weights\n",
    "    # For this example, we assume that all models have a single input data point, for simplicity.\n",
    "    # In a real scenario, the input to models would be the data they should predict.\n",
    "    weighted_avgs = model_predictions(gpp_model_1_Regridded_mean_shifted, gpp_model_2_Regridded_mean_shifted, gpp_model_3_Regridded_mean_shifted, gpp_model_4_Regridded_mean_shifted, gpp_model_5_Regridded_mean_shifted, gpp_model_6_Regridded_mean_shifted, gpp_model_7_Regridded_mean_shifted, gpp_model_8_Regridded_mean_shifted, gpp_model_9_Regridded_mean_shifted, gpp_model_10_Regridded_mean_shifted, new_weights[i, :])\n",
    "    \n",
    "    #Estimate RMSE for this BMA samples\n",
    "    rmse_weighted[i] = get_RMSE(weighted_avgs, data_obs) \n",
    "\n",
    "# Sorting RMSE to find the best one \n",
    "sorted_indices = np.argsort(rmse_weighted)\n",
    "best_rmse = rmse_weighted[sorted_indices[0]]\n",
    "best_location = sorted_indices[0]\n",
    "best_weights = new_weights[best_location, :]\n",
    "\n",
    "# Print to screen the best RMSE, Location of that sample, and the corresponding weights\n",
    "best_rmse, best_location, best_weights\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
