{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First steps for intake-ESGF-Catalog (https://intake-esgf.readthedocs.io/en/latest/quickstart.html)\n",
    "from intake_esgf import ESGFCatalog\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import netCDF4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined all functions used in this script\n",
    "\n",
    "# Placeholder function to generate a sample of weights\n",
    "def generate_sample_of_weights(num_models):\n",
    "    \"\"\"\n",
    "    Generate a sample of weights that are non-negative and sum up to 1.\n",
    "    Args:\n",
    "    num_models (int): The number of models (and hence the number of weights to generate)\n",
    "    Returns:\n",
    "    numpy.ndarray: An array of weights that sum up to 1.\n",
    "    \"\"\"\n",
    "    # Generate uniform samples which naturally fall between 0 and 1\n",
    "    weights = np.random.uniform(0, 1, size=num_models)\n",
    "    weights = weights **4\n",
    "    # Normalize the weights so they sum up to 1\n",
    "    normalized_weights = weights / np.sum(weights)\n",
    "    return normalized_weights\n",
    "\n",
    "# Placeholder for 'models' which would be an array of models.\n",
    "# Here we assume each model is a function that predicts an output given an input.\n",
    "# In the actual use case, these should be replaced with real predictive models.\n",
    "def model_predictions(models_1, models_2, models_3, models_4, models_5, models_6, models_7, models_8, models_9, models_10, weights_norm):\n",
    "    model_mean_weighted = (weights_norm[0]*models_1) + (weights_norm[1]*models_2) + (weights_norm[2]*models_3) + (weights_norm[3]* models_4) + (weights_norm[4]*models_5) + (weights_norm[5]*models_6) + (weights_norm[6]*models_7) + (weights_norm[7]*models_8) + (weights_norm[8]*models_9) + (weights_norm[9]*models_10)\n",
    "    return model_mean_weighted\n",
    "\n",
    "# function for shifiting lon according to Bharat's code\n",
    "def shift_lon(model):\n",
    "    # First shift the lon dimension for the model average\n",
    "    ds_tmp = model.copy(deep=True)\n",
    "    ds_tmp['lon'] = ds_tmp['lon'] - 180\n",
    "\n",
    "    gpp_reshape = np.zeros(ds_tmp.shape)\n",
    "    gpp_reshape[:,ds_tmp['lon'].size//2:] = ds_tmp[:,:ds_tmp['lon'].size//2].data\n",
    "    gpp_reshape[:,:ds_tmp['lon'].size//2] = ds_tmp[:,ds_tmp['lon'].size//2:].data\n",
    "    ds_tmp.data = gpp_reshape\n",
    "    return ds_tmp \n",
    "\n",
    "# Create function that will do RMSE for each seperate model\n",
    "\n",
    "def get_RMSE(model, obs):\n",
    "    # Step 2: Compute the Difference\n",
    "    difference = model - obs\n",
    "    # Step 3: Square the Difference\n",
    "    squared_difference = difference ** 2\n",
    "    # Step 4: Compute the Mean Squared Error\n",
    "    mse = squared_difference.mean(dim=['lat', 'lon'])\n",
    "    # Step 5: Take the Square Root to get RMSE\n",
    "    rmse  = np.sqrt(mse)\n",
    "    rmse_weighted = rmse.values.item()\n",
    "\n",
    "    return rmse_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform a search() to populate the catalog.\n"
     ]
    }
   ],
   "source": [
    "#Populate the Catalog - bringing in nothing from the catalog\n",
    "cat = ESGFCatalog()\n",
    "print(cat)  # <-- nothing to see here yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2edf026120c74137836bcbaf7a489747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   Searching indices:   0%|          |0/2 [       ?index/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Summary information for 10 results:\n",
       "mip_era                                                     [CMIP6]\n",
       "experiment_id                                          [historical]\n",
       "variable_id                                                   [gpp]\n",
       "grid_label                                            [gr1, gn, gr]\n",
       "institution_id    [NOAA-GFDL, NCC, IPSL, CSIRO, NCAR, BCC, MOHC,...\n",
       "member_id                                      [r1i1p1f1, r1i1p1f2]\n",
       "table_id                                                     [Lmon]\n",
       "source_id         [GFDL-ESM4, NorESM2-LM, IPSL-CM6A-LR, ACCESS-E...\n",
       "activity_drs                                                 [CMIP]\n",
       "project                                                     [CMIP6]\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import selected models from Intake-ESGF Catalog for selected variable\n",
    "\n",
    "models = [\"ACCESS-ESM1-5\",\"IPSL-CM6A-LR\",\"CESM2\", \"UKESM1-0-LL\",\"BCC-CSM2-MR\",\"MPI-ESM1-2-HR\",\"CanESM5\",\"GFDL-ESM4\",\"NorESM2-LM\", \"MIROC-ES2L\"]\n",
    "\n",
    "cat.search(\n",
    "    experiment_id=\"historical\",\n",
    "    source_id= models,\n",
    "    frequency=\"mon\",\n",
    "    variable_id=[\"gpp\"],\n",
    ")\n",
    "cat.remove_ensembles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb2c58b13dd45d4a7aab6e04db65679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Get file information:   0%|          |0/2 [       ?index/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04afc821cbb843cb9f9e678cbf140a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding cell measures:   0%|          |0/10 [     ?dataset/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Obtaining the datasets and loading it into a dictionary (putting it in the shopping cart)\n",
    "\n",
    "dsd = cat.to_dataset_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['gn.NCC.r1i1p1f1.NorESM2-LM', 'gn.MPI-M.r1i1p1f1.MPI-ESM1-2-HR', 'gr.IPSL.r1i1p1f1.IPSL-CM6A-LR', 'gn.MIROC.r1i1p1f2.MIROC-ES2L', 'gn.NCAR.r1i1p1f1.CESM2', 'gn.CCCma.r1i1p1f1.CanESM5', 'gn.MOHC.r1i1p1f2.UKESM1-0-LL', 'gr1.NOAA-GFDL.r1i1p1f1.GFDL-ESM4', 'gn.BCC.r1i1p1f1.BCC-CSM2-MR', 'gn.CSIRO.r1i1p1f1.ACCESS-ESM1-5'])\n"
     ]
    }
   ],
   "source": [
    "#printing variable keys to see how variable names are set up\n",
    "print(dsd.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 gn.CCCma.r1i1p1f1.CanESM5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# # Define models for selected time periods and fixed units (make sure that model names match the printed dict_key (above))\n",
    "# list_keys = (list(dsd.keys()))\n",
    "\n",
    "# model_1 = dsd[list_keys[0]][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400 * 1000 # .mean(dim=\"time\") * 86400 * 1000\n",
    "# model_2 = dsd[list_keys[1]][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400 * 1000 # .mean(dim=\"time\") * 86400 * 1000\n",
    "# model_3 = dsd[list_keys[2]][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400 * 1000 # .mean(dim=\"time\") * 86400 * 1000\n",
    "# model_4 = dsd[list_keys[3]][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400 * 1000 # .mean(dim=\"time\") * 86400 * 1000\n",
    "# model_5 = dsd[list_keys[4]][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400 * 1000 # .mean(dim=\"time\") * 86400 * 1000\n",
    "# model_6 = dsd[list_keys[5]][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400 * 1000 # .mean(dim=\"time\") * 86400 * 1000\n",
    "# model_7 = dsd[list_keys[6]][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400 * 1000 # .mean(dim=\"time\") * 86400 * 1000\n",
    "# model_8 = dsd[list_keys[7]][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400 * 1000 # .mean(dim=\"time\") * 86400 * 1000\n",
    "# model_9 = dsd[list_keys[8]][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400 * 1000 # .mean(dim=\"time\") * 86400 * 1000\n",
    "# model_10 = dsd[list_keys[9]][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400 * 1000 # .mean(dim=\"time\") * 86400 * 1000\n",
    "\n",
    "# # Find out which model is CanESM (lowest resolution fo regridding)\n",
    "# for idx, item in enumerate(list_keys):\n",
    "#     if 'CanESM' in item:\n",
    "#         print(idx, item)\n",
    "#         break\n",
    "\n",
    "# model_CanESM_id = idx\n",
    "# print(model_CanESM_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 gn.CCCma.r1i1p1f1.CanESM5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Load each model in its own dataset\n",
    "\n",
    "# list keys of model names\n",
    "list_keys = (list(dsd.keys()))\n",
    "\n",
    "# Find out which model to load weights into\n",
    "for idx, item in enumerate(list_keys):\n",
    "    if 'ACCESS' in item:\n",
    "        model_1 = dsd[list_keys[idx]][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400 * 1000 # .mean(dim=\"time\") * 86400 * 1000\n",
    "    if 'BCC' in item:\n",
    "        model_2 = dsd[list_keys[idx]][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400 * 1000 # .mean(dim=\"time\") * 86400 * 1000\n",
    "    if 'CanESM5' in item:\n",
    "        model_3 = dsd[list_keys[idx]][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400 * 1000 # .mean(dim=\"time\") * 86400 * 1000\n",
    "    if 'CESM2' in item:\n",
    "        model_4 = dsd[list_keys[idx]][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400 * 1000 # .mean(dim=\"time\") * 86400 * 1000\n",
    "    if 'GFDL' in item:\n",
    "        model_5 = dsd[list_keys[idx]][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400 * 1000 # .mean(dim=\"time\") * 86400 * 1000\n",
    "    if 'IPSL' in item:\n",
    "        model_6 = dsd[list_keys[idx]][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400 * 1000 # .mean(dim=\"time\") * 86400 * 1000\n",
    "    if 'MIROC' in item:\n",
    "        model_7 = dsd[list_keys[idx]][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400 * 1000 # .mean(dim=\"time\") * 86400 * 1000\n",
    "    if 'MPI' in item:\n",
    "        model_8 = dsd[list_keys[idx]][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400 * 1000 # .mean(dim=\"time\") * 86400 * 1000\n",
    "    if 'NorESM2' in item:\n",
    "        model_9 = dsd[list_keys[idx]][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400 * 1000 # .mean(dim=\"time\") * 86400 * 1000\n",
    "    if 'UKESM1' in item:\n",
    "        model_10 = dsd[list_keys[idx]][\"gpp\"].sel(time=slice('1980-01-01', '2013-12-01'))* 86400 * 1000 # .mean(dim=\"time\") * 86400 * 1000\n",
    "\n",
    "# Find out which model is CanESM (lowest resolution fo regridding)\n",
    "for idx, item in enumerate(list_keys):\n",
    "    if 'CanESM' in item:\n",
    "        print(idx, item)\n",
    "        break\n",
    "\n",
    "model_CanESM_id = idx\n",
    "print(model_CanESM_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Interpolation/Regridding to match lowest resolution model (CanESM5):\n",
    "\n",
    "#Extract lat/lon grid from CanESM5\n",
    "lat_target = dsd[list_keys[idx]][\"gpp\"].lat.values\n",
    "lon_target = dsd[list_keys[idx]][\"gpp\"].lon.values\n",
    "\n",
    "# Regrid each model and take long (not longtitude) term mean \n",
    "gpp_model_1_Regridded = model_1.interp(lat=lat_target, lon=lon_target)\n",
    "gpp_model_1_Regridded_mean = gpp_model_1_Regridded.mean(dim=\"time\")\n",
    "\n",
    "gpp_model_2_Regridded = model_2.interp(lat=lat_target, lon=lon_target)\n",
    "gpp_model_2_Regridded_mean = gpp_model_2_Regridded.mean(dim=\"time\")\n",
    "\n",
    "gpp_model_3_Regridded = model_3.interp(lat=lat_target, lon=lon_target)\n",
    "gpp_model_3_Regridded_mean = gpp_model_3_Regridded.mean(dim=\"time\")\n",
    "\n",
    "gpp_model_4_Regridded = model_4.interp(lat=lat_target, lon=lon_target)\n",
    "gpp_model_4_Regridded_mean = gpp_model_4_Regridded.mean(dim=\"time\")\n",
    "\n",
    "gpp_model_5_Regridded = model_5.interp(lat=lat_target, lon=lon_target)\n",
    "gpp_model_5_Regridded_mean = gpp_model_5_Regridded.mean(dim=\"time\")\n",
    "\n",
    "gpp_model_6_Regridded = model_6.interp(lat=lat_target, lon=lon_target)\n",
    "gpp_model_6_Regridded_mean = gpp_model_6_Regridded.mean(dim=\"time\")\n",
    "\n",
    "gpp_model_7_Regridded = model_7.interp(lat=lat_target, lon=lon_target)\n",
    "gpp_model_7_Regridded_mean = gpp_model_7_Regridded.mean(dim=\"time\")\n",
    "\n",
    "gpp_model_8_Regridded = model_8.interp(lat=lat_target, lon=lon_target)\n",
    "gpp_model_8_Regridded_mean = gpp_model_8_Regridded.mean(dim=\"time\")\n",
    "\n",
    "gpp_model_9_Regridded = model_9.interp(lat=lat_target, lon=lon_target)\n",
    "gpp_model_9_Regridded_mean = gpp_model_9_Regridded.mean(dim=\"time\")\n",
    "\n",
    "gpp_model_10_Regridded = model_10.interp(lat=lat_target, lon=lon_target)\n",
    "gpp_model_10_Regridded_mean = gpp_model_10_Regridded.mean(dim=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift every model to proper coordinates\n",
    "gpp_model_1_Regridded_mean_shifted = shift_lon(gpp_model_1_Regridded_mean)\n",
    "gpp_model_2_Regridded_mean_shifted = shift_lon(gpp_model_2_Regridded_mean)\n",
    "gpp_model_3_Regridded_mean_shifted = shift_lon(gpp_model_3_Regridded_mean)\n",
    "gpp_model_4_Regridded_mean_shifted = shift_lon(gpp_model_4_Regridded_mean)\n",
    "gpp_model_5_Regridded_mean_shifted = shift_lon(gpp_model_5_Regridded_mean)\n",
    "gpp_model_6_Regridded_mean_shifted = shift_lon(gpp_model_6_Regridded_mean)\n",
    "gpp_model_7_Regridded_mean_shifted = shift_lon(gpp_model_7_Regridded_mean)\n",
    "gpp_model_8_Regridded_mean_shifted = shift_lon(gpp_model_8_Regridded_mean)\n",
    "gpp_model_9_Regridded_mean_shifted = shift_lon(gpp_model_9_Regridded_mean)\n",
    "gpp_model_10_Regridded_mean_shifted = shift_lon(gpp_model_10_Regridded_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the observation \n",
    "\n",
    "# Open the NetCDF file as an xarray Dataset\n",
    "ds = xr.open_dataset('/Users/6i0/Documents/Data/gpp_WECANN.nc')\n",
    "\n",
    "# Access the GPP variable from the Dataset\n",
    "gpp_data = ds['gpp']\n",
    "gpp_data_mean = gpp_data.mean(dim=\"time\")\n",
    "lat = ds['lat']\n",
    "lon = ds['lon']\n",
    "time = ds['time']\n",
    "\n",
    "#gpp_data_Regridded_mean = gpp_data_mean\n",
    "lat_target = dsd[list_keys[idx]][\"gpp\"].lat.values\n",
    "lon_target = dsd[list_keys[idx]][\"gpp\"].lon.values -180\n",
    "\n",
    "# Correctly using ds_emean for interpolation\n",
    "gpp_data_Regridded = gpp_data.interp(lat=lat_target, lon=lon_target)\n",
    "gpp_data_Regridded_mean = gpp_data_Regridded.mean(dim=\"time\")\n",
    "\n",
    "#Shift data\n",
    "gpp_data_Regridded_mean_shifted = shift_lon(gpp_data_Regridded_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gn.NCC.r1i1p1f1.NorESM2-LM', 'gn.MPI-M.r1i1p1f1.MPI-ESM1-2-HR', 'gr.IPSL.r1i1p1f1.IPSL-CM6A-LR', 'gn.MIROC.r1i1p1f2.MIROC-ES2L', 'gn.NCAR.r1i1p1f1.CESM2', 'gn.CCCma.r1i1p1f1.CanESM5', 'gn.MOHC.r1i1p1f2.UKESM1-0-LL', 'gr1.NOAA-GFDL.r1i1p1f1.GFDL-ESM4', 'gn.BCC.r1i1p1f1.BCC-CSM2-MR', 'gn.CSIRO.r1i1p1f1.ACCESS-ESM1-5']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5536644728234702,\n",
       " 5677,\n",
       " array([0.04999625, 0.00140119, 0.18983852, 0.07555226, 0.12824551,\n",
       "        0.22720795, 0.17796318, 0.10856709, 0.04043152, 0.00079654]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BMA Implementation:\n",
    "\n",
    "# Placeholder for actual observed data (data_obs)\n",
    "data_obs = gpp_data_Regridded_mean\n",
    "\n",
    "# Number of models we are combining\n",
    "num_models = 10 \n",
    "# Number of BMA Samples \n",
    "n_samples= 10000\n",
    "\n",
    "# Initialize arrays to hold weights, weighted averages and RMSE\n",
    "new_weights = np.zeros((n_samples, num_models))\n",
    "weighted_avgs = np.zeros((n_samples, len(data_obs)))\n",
    "rmse_weighted = np.zeros(n_samples)\n",
    "\n",
    "# Loop to generate weights, calculate weighted averages and RMSE\n",
    "for i in range(n_samples):\n",
    "    weights = generate_sample_of_weights(num_models)\n",
    "    new_weights[i, :] = weights\n",
    "    # For this example, we assume that all models have a single input data point, for simplicity.\n",
    "    # In a real scenario, the input to models would be the data they should predict.\n",
    "    weighted_avgs = model_predictions(gpp_model_1_Regridded_mean_shifted, gpp_model_2_Regridded_mean_shifted, gpp_model_3_Regridded_mean_shifted, gpp_model_4_Regridded_mean_shifted, gpp_model_5_Regridded_mean_shifted, gpp_model_6_Regridded_mean_shifted, gpp_model_7_Regridded_mean_shifted, gpp_model_8_Regridded_mean_shifted, gpp_model_9_Regridded_mean_shifted, gpp_model_10_Regridded_mean_shifted, new_weights[i, :])\n",
    "    \n",
    "    #Estimate RMSE for this BMA samples\n",
    "    rmse_weighted[i] = get_RMSE(weighted_avgs, data_obs) \n",
    "\n",
    "# Sorting RMSE to find the best one \n",
    "sorted_indices = np.argsort(rmse_weighted)\n",
    "best_rmse = rmse_weighted[sorted_indices[0]]\n",
    "best_location = sorted_indices[0]\n",
    "best_weights = new_weights[best_location, :]\n",
    "\n",
    "# Print to screen the best RMSE, Location of that sample, and the corresponding weights\n",
    "print(list_keys)\n",
    "best_rmse, best_location, best_weights\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
